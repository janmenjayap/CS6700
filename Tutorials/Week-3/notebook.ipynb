{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptrfMg83-IXT"
      },
      "source": [
        "# CS6700: Reinforcement Learning - Tutorial 1 (MENACE)\n",
        "\n",
        "#### Tasks\n",
        "1. Complete code to determine if there is a winner at a particular state\n",
        "2. Complete code to update state-action values of a player based on play history\n",
        "3. Plot win, draw and loss %ages while training MENACE vs MENACE\n",
        "4. Plot win, draw and loss %ages while training MENACE vs Random policy\n",
        "5. Report any observations and inferences from the plots in 3 & 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LD5-1Iw8PafI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import NamedTuple\n",
        "# from google.colab import output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "I04JDLKvQwPo"
      },
      "outputs": [],
      "source": [
        "SEED = 0\n",
        "NUM_EPOCHS = 1_00_000\n",
        "\n",
        "BOARD_COL = 3\n",
        "BOARD_ROW = 3\n",
        "BOARD_SIZE = BOARD_COL * BOARD_ROW\n",
        "\n",
        "\"\"\"\n",
        "Game board and actions are: {q, w, e, a, s, d, z, x, c}\n",
        "\n",
        "q | w | e\n",
        "--|---|--\n",
        "a | s | d\n",
        "--|---|--\n",
        "z | x | c\n",
        "\"\"\"\n",
        "ACTIONS_KEY_MAP = {'q': 0, 'w': 1, 'e': 2,\n",
        "                   'a': 3, 's': 4, 'd': 5,\n",
        "                   'z': 6, 'x': 7, 'c': 8}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "n7lTbDhBy5Of"
      },
      "outputs": [],
      "source": [
        "np.random.seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQhLYLOByy-D"
      },
      "source": [
        "## State Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IFBWExhtRAMR"
      },
      "outputs": [],
      "source": [
        "def print_state(board, clear_output=False):\n",
        "  # if clear_output:\n",
        "    # output.clear()\n",
        "  for i in range(BOARD_ROW):\n",
        "    print('-------------')\n",
        "    out = '| '\n",
        "    for j in range(BOARD_COL):\n",
        "      if board[i, j] == 1:\n",
        "          token = 'x'\n",
        "      elif board[i, j] == -1:\n",
        "          token = 'o'\n",
        "      else:\n",
        "          token = ' '  # empty position\n",
        "      out += token + ' | '\n",
        "    print(out)\n",
        "  print('-------------')\n",
        "\n",
        "\n",
        "class State:\n",
        "  def __init__(self, symbol):\n",
        "    # the board is represented by an n * n array,\n",
        "    #  1 represents the player who moves first (X),\n",
        "    # -1 represents another player (O)\n",
        "    #  0 represents an empty position\n",
        "    self.board = np.zeros((BOARD_ROW, BOARD_COL))\n",
        "    self.symbol = symbol\n",
        "    self.winner = 0\n",
        "    self.end = None\n",
        "\n",
        "  @property\n",
        "  def hash_value(self):\n",
        "    hash = 0\n",
        "    for x in np.nditer(self.board):\n",
        "      hash = 3*hash + x + 1   # unique hash\n",
        "    return hash\n",
        "\n",
        "  def next(self, action: str):\n",
        "    id = ACTIONS_KEY_MAP[action]\n",
        "    i, j = id // BOARD_COL, id % BOARD_COL\n",
        "    return self.next_by_pos(i, j)\n",
        "\n",
        "  def next_by_pos(self, i: int, j: int):\n",
        "    assert self.board[i, j] == 0\n",
        "    new_state = State(-self.symbol)      # another player turn\n",
        "    new_state.board = np.copy(self.board)\n",
        "    new_state.board[i, j] = self.symbol  # current player choose to play at (i, j) pos\n",
        "    return new_state\n",
        "\n",
        "  @property\n",
        "  def possible_actions(self):\n",
        "    rev_action_map = {id: key for key, id in ACTIONS_KEY_MAP.items()}\n",
        "    actions = []\n",
        "    for i in range(BOARD_ROW):\n",
        "      for j in range(BOARD_COL):\n",
        "        if self.board[i, j] == 0:\n",
        "          actions.append(rev_action_map[BOARD_COL*i+j])\n",
        "    return actions\n",
        "\n",
        "  def is_end(self):\n",
        "    if self.end is not None:\n",
        "      return self.end\n",
        "\n",
        "\n",
        "    ### WRITE YOUR CODE HERE ###\n",
        "    # check 3 rows, 3 columns and both diagonals\n",
        "    # check if the state is an end state\n",
        "    # set self.end to be True when the game has ended\n",
        "    # set self.winner to be 0 (draw), 1 (player 1) or 2 (player 2)\n",
        "    def check_winner_in_line(line):\n",
        "      self.winner = 1 if all(x == 1 for x in line) else 2 if all(x == -1 for x in line) else 0\n",
        "      self.end = (self.winner != 0)\n",
        "      return self.end\n",
        "\n",
        "    for i in range(BOARD_ROW):\n",
        "        if check_winner_in_line(self.board[i, :]) or check_winner_in_line(self.board[:, i]):\n",
        "            return self.end\n",
        "\n",
        "    if check_winner_in_line(self.board.diagonal()) or check_winner_in_line(np.fliplr(self.board).diagonal()):\n",
        "        return self.end\n",
        "\n",
        "\n",
        "    # if there is no winner\n",
        "    # check if there are any available plays\n",
        "    for x in np.nditer(self.board):\n",
        "      if x == 0:\n",
        "        self.end = False\n",
        "        return self.end\n",
        "\n",
        "    # declare a draw\n",
        "    self.winner = 0\n",
        "    self.end = True\n",
        "    return self.end"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihwyqTpPy2H2"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XJG-IRKIDsz9"
      },
      "outputs": [],
      "source": [
        "class Env:\n",
        "  def __init__(self):\n",
        "    self.all_states = self.get_all_states()\n",
        "    self.curr_state = State(symbol=1)\n",
        "\n",
        "  def get_all_states(self):\n",
        "    all_states = {}  # is a dict with key as state_hash_value and value as State object.\n",
        "    def explore_all_substates(state):\n",
        "      for i in range(BOARD_ROW):\n",
        "        for j in range(BOARD_COL):\n",
        "          if state.board[i, j] == 0:\n",
        "            next_state = state.next_by_pos(i, j)\n",
        "            if next_state.hash_value not in all_states:\n",
        "              all_states[next_state.hash_value] = next_state\n",
        "              if not next_state.is_end():\n",
        "                explore_all_substates(next_state)\n",
        "    curr_state = State(symbol=1)\n",
        "    all_states[curr_state.hash_value] = curr_state\n",
        "    explore_all_substates(curr_state)\n",
        "    return all_states\n",
        "\n",
        "  def reset(self):\n",
        "    self.curr_state = State(symbol=1)\n",
        "    return self.curr_state\n",
        "\n",
        "  def step(self, action):\n",
        "    assert action in self.curr_state.possible_actions, f\"Invalid {action} for the current state \\n{self.curr_state.print_state()}\"\n",
        "    next_state_hash = self.curr_state.next(action).hash_value\n",
        "    next_state = self.all_states[next_state_hash]\n",
        "    self.curr_state = next_state\n",
        "    reward = 0\n",
        "    return self.curr_state, reward\n",
        "\n",
        "  def is_end(self):\n",
        "    return self.curr_state.is_end()\n",
        "\n",
        "  @property\n",
        "  def winner(self):\n",
        "    result_id = self.curr_state.winner\n",
        "    result = 'draw'\n",
        "    if result_id == 1:\n",
        "      result = 'player1'\n",
        "    elif result_id == 2:\n",
        "      result = 'player2'\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZKfXapCzHNq"
      },
      "source": [
        "## Policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pN3u_Ss0lf88"
      },
      "outputs": [],
      "source": [
        "class BasePolicy:\n",
        "  def reset(self):\n",
        "    pass\n",
        "\n",
        "  def update_values(self, *args):\n",
        "    pass\n",
        "\n",
        "  def select_action(self, state):\n",
        "    raise Exception('Not Implemented Error')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OfddswzCRK8p"
      },
      "outputs": [],
      "source": [
        "class HumanPolicy(BasePolicy):\n",
        "  def __init__(self, symbol):\n",
        "    self.symbol = symbol\n",
        "\n",
        "  def select_action(self, state):\n",
        "    assert state.symbol == self.symbol, f\"Its not {self.symbol} symbol's turn\"\n",
        "    print_state(state.board, clear_output=True)\n",
        "    key = input(\"Input your position: \")\n",
        "    return key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kJ_18UpiogzN"
      },
      "outputs": [],
      "source": [
        "class RandomPolicy(BasePolicy):\n",
        "  def __init__(self, symbol):\n",
        "    self.symbol = symbol\n",
        "\n",
        "  def select_action(self, state):\n",
        "    assert state.symbol == self.symbol, f\"Its not {self.symbol} symbol's turn\"\n",
        "    return np.random.choice(state.possible_actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mos8O8H1RFe5"
      },
      "outputs": [],
      "source": [
        "class ActionPlayed(NamedTuple):\n",
        "  hash_value: str\n",
        "  action: str\n",
        "\n",
        "\n",
        "class MenacePolicy(BasePolicy):\n",
        "  def __init__(self, all_states, symbol, tau=5.0):\n",
        "    self.all_states = all_states\n",
        "    self.symbol = symbol\n",
        "    self.tau = tau\n",
        "\n",
        "    # It store the number of stones for each action for each state\n",
        "    self.state_action_value = self.initialize()\n",
        "    # variable to store the history for updating the number of stones\n",
        "    self.history = []\n",
        "\n",
        "  def initialize(self):\n",
        "    state_action_value = {}\n",
        "    for hash_value, state in self.all_states.items():\n",
        "      # initially all actions have 0 stones\n",
        "      state_action_value[hash_value] = {action: 0 for action in state.possible_actions}\n",
        "    return state_action_value\n",
        "\n",
        "  def reset(self):\n",
        "    for action_value in self.state_action_value.values():\n",
        "      for action in action_value.keys():\n",
        "        action_value[action] = 0\n",
        "\n",
        "  def print_updates(self, reward):\n",
        "    print(f'Player with symbol {self.symbol} updates the following history with {reward} stone')\n",
        "    for item in self.history:\n",
        "      board = np.copy(self.all_states[item.hash_value].board)\n",
        "      id = ACTIONS_KEY_MAP[item.action]\n",
        "      i, j = id//BOARD_COL, id%BOARD_COL\n",
        "      board[i, j] = self.symbol\n",
        "      print_state(board)\n",
        "\n",
        "  def update_values(self, reward, show_update=False):\n",
        "    # reward: if wins receive reward of 1 stone for the chosen action\n",
        "    #         else -1 stone.\n",
        "    # reward is either 1 or -1 depending upon if the player has won or lost the game.\n",
        "\n",
        "    if show_update:\n",
        "      self.print_updates(reward)\n",
        "\n",
        "    # for every state-action in history\n",
        "    # use reward to update the state-action values\n",
        "    ### WRITE CODE HERE\n",
        "    for item in self.history:\n",
        "      self.state_action_value[item.hash_value][item.action] += reward\n",
        "\n",
        "    self.history = []\n",
        "\n",
        "  def select_action(self, state):  # Softmax action probability\n",
        "    assert state.symbol == self.symbol, f\"Its not {self.symbol} symbol's turn\"\n",
        "    action_value = self.state_action_value[state.hash_value]\n",
        "    max_value = action_value[max(action_value, key=action_value.get)]\n",
        "    exp_values = {action: np.exp((v-max_value) / self.tau) for action, v in action_value.items()}\n",
        "    normalizer = np.sum([v for v in exp_values.values()])\n",
        "    prob = {action: v/normalizer for action, v in exp_values.items()}\n",
        "    action = np.random.choice(list(prob.keys()), p=list(prob.values()))\n",
        "    self.history.append(ActionPlayed(state.hash_value, action))\n",
        "    return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MvkDRmozqWM"
      },
      "source": [
        "## Game Board"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qzO3FQJcCqpJ"
      },
      "outputs": [],
      "source": [
        "class Game:\n",
        "  def __init__(self, env, player1, player2):\n",
        "    self.env = env\n",
        "    self.player1 = player1\n",
        "    self.player2 = player2\n",
        "    self.show_updates = False\n",
        "    self.train_results = None\n",
        "\n",
        "  def alternate(self):\n",
        "    while True:\n",
        "      yield self.player1\n",
        "      yield self.player2\n",
        "\n",
        "  def train(self, epochs=1_00_000):\n",
        "    self.train_results = [[], []]\n",
        "    player1_reward_map = {'player1': 1, 'player2': -1, 'draw': 0}\n",
        "    for _ in range(epochs):\n",
        "      result = self.play()\n",
        "\n",
        "      # if player1 wins add 1 stone for the action chosen\n",
        "      player1_reward = player1_reward_map[result]\n",
        "      player2_reward = -player1_reward   # if player2 wins add 1 stone\n",
        "\n",
        "      self.player1.update_values(player1_reward)\n",
        "      self.player2.update_values(player2_reward)\n",
        "\n",
        "      # append results\n",
        "      self.train_results[0].append(player1_reward)\n",
        "      self.train_results[1].append(player2_reward)\n",
        "\n",
        "  def play(self):\n",
        "    alternate = self.alternate()\n",
        "    state = self.env.reset()\n",
        "    while not self.env.is_end():\n",
        "      player = next(alternate)\n",
        "      action = player.select_action(state)\n",
        "      state, _ = self.env.step(action)\n",
        "    result = self.env.winner\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-gqpWkKztLm"
      },
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Pr62H-hLLYdt"
      },
      "outputs": [],
      "source": [
        "env = Env()\n",
        "\n",
        "# Game 1: train MENACE vs MENACE\n",
        "# plot win, draw, loss fractions for player 1\n",
        "player1 = MenacePolicy(env.all_states, symbol=1)\n",
        "player2 = MenacePolicy(env.all_states, symbol=-1)\n",
        "game1 = Game(env, player1, player2)\n",
        "game1.train(epochs=NUM_EPOCHS)\n",
        "\n",
        "# GAME 2: train MENACE vs RANDOM\n",
        "# plot win, draw, loss fractions for player 3\n",
        "player3 = MenacePolicy(env.all_states, symbol=1)\n",
        "player4 = RandomPolicy(symbol=-1)\n",
        "game2 = Game(env, player3, player4)\n",
        "game2.train(epochs=NUM_EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "INfZuvYiJsh_",
        "outputId": "29a56c54-64b5-4018-f6af-7b5be37a000e"
      },
      "outputs": [],
      "source": [
        "results1 = game1.train_results[0]\n",
        "wins1, draws1, losses1, tot1 = 0., 0., 0., 0.\n",
        "fracs1 = [[], [], []]\n",
        "for i in range(NUM_EPOCHS):\n",
        "  tot1 += 1\n",
        "  if results1[i] == 1: wins1 += 1\n",
        "  elif results1[i] == 0: draws1 += 1\n",
        "  else: losses1 += 1\n",
        "\n",
        "  fracs1[0].append((wins1/tot1)*100)\n",
        "  fracs1[1].append((losses1/tot1)*100)\n",
        "  fracs1[2].append((draws1/tot1)*100)\n",
        "\n",
        "plt.plot(range(NUM_EPOCHS), fracs1[0], label = 'wins')\n",
        "plt.plot(range(NUM_EPOCHS), fracs1[1], label = 'losses')\n",
        "plt.plot(range(NUM_EPOCHS), fracs1[2], label = 'draws')\n",
        "plt.title('Win-Loss-Draw percentages for Player 1 (MENACE trained vs MENACE)')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "pJLVk_xKPEmS",
        "outputId": "185c10be-4a60-4fa9-a29f-76ba72695ea7"
      },
      "outputs": [],
      "source": [
        "results2 = game2.train_results[0]\n",
        "wins2, draws2, losses2, tot2 = 0., 0., 0., 0.\n",
        "fracs2 = [[], [], []]\n",
        "for i in range(NUM_EPOCHS):\n",
        "  tot2 += 1\n",
        "  if results2[i] == 1: wins2 += 1\n",
        "  elif results2[i] == 0: draws2 += 1\n",
        "  else: losses2 += 1\n",
        "\n",
        "  fracs2[0].append((wins2/tot2)*100)\n",
        "  fracs2[1].append((losses2/tot2)*100)\n",
        "  fracs2[2].append((draws2/tot2)*100)\n",
        "\n",
        "plt.plot(range(NUM_EPOCHS), fracs2[0], label = 'wins')\n",
        "plt.plot(range(NUM_EPOCHS), fracs2[1], label = 'losses')\n",
        "plt.plot(range(NUM_EPOCHS), fracs2[2], label = 'draws')\n",
        "plt.title('Win-Loss-Draw percentages for Player 3 (MENACE trained vs Random)')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMbqaln6TDXZ",
        "outputId": "96e4490c-f6f5-4e1c-ce8d-9bf95d3d0e67"
      },
      "outputs": [],
      "source": [
        "# GAME 3: Play against player 1 (MENACE trained vs MENACE)\n",
        "# See if you can beat this policy!\n",
        "\n",
        "game3 = Game(env, player1, HumanPolicy(symbol=-1))\n",
        "game3.play()\n",
        "\n",
        "result = env.winner\n",
        "print(f\"winner: {result}\")\n",
        "\n",
        "player1_reward_map = {'player1': 1, 'player2': -1, 'draw': 0}\n",
        "player1.update_values(player1_reward_map[result], show_update=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgNksx7ZQrGU"
      },
      "source": [
        "#### Question\n",
        "\n",
        "What can you infer from the above series of experiments?\n",
        "\n",
        "**ENTER ANSWER HERE**\n",
        "\n",
        "<hr/>\n",
        "Observe that, the machine learns how to play properly by identifying the better moves through its training. This can be said as when we go through MENACE trained vs human with the human making the second move, if the latter does not places a 'o' at the central sqaure, MENACE tends to win those game absolutely easily like a professional player. When the game is played between MENACE trained and MENACE, as both of them learn sucessively and play better, as the number of games tends to infinity, the percentage of draws tends to 1. On the other hand, it can be seen that, a MENACE tranined system can effortlessly beat a random-choice strategy.\n",
        "\n",
        "\n",
        "The main lesson here is to prioritise and identify the good set of state-action pair throught sucessive game plays and that way the reinforcement gets stronger. This is a very great way of training models especially when the entire game-tree itself is quite small for a game like tic-tac-toe. But this kind of strategy will be computational unimaginably expensive for a game involving significant amount of state-action pairs for example chess, go etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Dmt8VOuiWok"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
