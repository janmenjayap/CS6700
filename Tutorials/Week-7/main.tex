\documentclass[11pt]{article}
    \author{ME20B087 Janmenjaya Panda}
    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    
    \usepackage{iftex}
    \ifPDFTeX
    	\usepackage[T1]{fontenc}
    	\usepackage{mathpazo}
    \else
    	\usepackage{fontspec}
    \fi

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}
    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage{fancyvrb} % verbatim replacement that allows latex

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{Tutorial-5}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@ges}{\let\PY@bf=\textbf\let\PY@it=\textit}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb. 
    \makeatletter
        \newbox\Wrappedcontinuationbox 
        \newbox\Wrappedvisiblespacebox 
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}} 
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}} 
        \newcommand*\Wrappedcontinuationindent {3ex } 
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox} 
        % Take advantage of the already applied Pygments mark-up to insert 
        % potential linebreaks for TeX processing. 
        %        {, <, #, %, $, ' and ": go to next line. 
        %        _, }, ^, &, >, - and ~: stay at end of broken line. 
        % Use of \textquotesingle for straight quote. 
        \newcommand*\Wrappedbreaksatspecials {% 
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}% 
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}% 
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}% 
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}% 
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}% 
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}% 
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}% 
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}% 
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}% 
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}% 
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}% 
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}% 
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}% 
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}% 
        } 
        % Some characters . , ; ? ! / are not pygmentized. 
        % This macro makes them "active" and they will insert potential linebreaks 
        \newcommand*\Wrappedbreaksatpunct {% 
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}% 
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}% 
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}% 
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}% 
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}% 
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}% 
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}% 
            \catcode`\.\active
            \catcode`\,\active 
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active 
            \lccode`\~`\~ 	
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%
        
        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active 	
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}
    
    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        \ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \#Tutorial 5 - DQN

Please follow this tutorial to understand the structure (code) of DQN
algorithm.

\hypertarget{references}{%
\subsubsection{References:}\label{references}}

Please follow
\href{https://www.nature.com/articles/nature14236}{Human-level control
through deep reinforcement learning} for the original publication as
well as the psuedocode. Watch Prof.~Ravi's lectures on moodle or nptel
for further understanding of the core concepts. Contact the TAs for
further resources if needed.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
\PY{l+s+sd}{Installing packages for rendering the game on Colab}
\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}

\PY{o}{!}pip\PY{+w}{ }install\PY{+w}{ }gym\PY{+w}{ }pyvirtualdisplay\PY{+w}{ }\PYZgt{}\PY{+w}{ }/dev/null\PY{+w}{ }\PY{l+m}{2}\PYZgt{}\PY{p}{\PYZam{}}\PY{l+m}{1}
\PY{o}{!}apt\PYZhy{}get\PY{+w}{ }install\PY{+w}{ }\PYZhy{}y\PY{+w}{ }xvfb\PY{+w}{ }python\PYZhy{}opengl\PY{+w}{ }ffmpeg\PY{+w}{ }\PYZgt{}\PY{+w}{ }/dev/null\PY{+w}{ }\PY{l+m}{2}\PYZgt{}\PY{p}{\PYZam{}}\PY{l+m}{1}
\PY{o}{!}apt\PYZhy{}get\PY{+w}{ }update\PY{+w}{ }\PYZgt{}\PY{+w}{ }/dev/null\PY{+w}{ }\PY{l+m}{2}\PYZgt{}\PY{p}{\PYZam{}}\PY{l+m}{1}
\PY{o}{!}apt\PYZhy{}get\PY{+w}{ }install\PY{+w}{ }cmake\PY{+w}{ }\PYZgt{}\PY{+w}{ }/dev/null\PY{+w}{ }\PY{l+m}{2}\PYZgt{}\PY{p}{\PYZam{}}\PY{l+m}{1}
\PY{o}{!}pip\PY{+w}{ }install\PY{+w}{ }\PYZhy{}\PYZhy{}upgrade\PY{+w}{ }setuptools\PY{+w}{ }\PY{l+m}{2}\PYZgt{}\PY{p}{\PYZam{}}\PY{l+m}{1}
\PY{o}{!}pip\PY{+w}{ }install\PY{+w}{ }ez\PYZus{}setup\PY{+w}{ }\PYZgt{}\PY{+w}{ }/dev/null\PY{+w}{ }\PY{l+m}{2}\PYZgt{}\PY{p}{\PYZam{}}\PY{l+m}{1}
\PY{o}{!}pip\PY{+w}{ }install\PY{+w}{ }gym\PY{o}{[}atari\PY{o}{]}\PY{+w}{ }\PYZgt{}\PY{+w}{ }/dev/null\PY{+w}{ }\PY{l+m}{2}\PYZgt{}\PY{p}{\PYZam{}}\PY{l+m}{1}
\PY{o}{!}pip\PY{+w}{ }install\PY{+w}{ }git+https://github.com/tensorflow/docs\PY{+w}{ }\PYZgt{}\PY{+w}{ }/dev/null\PY{+w}{ }\PY{l+m}{2}\PYZgt{}\PY{p}{\PYZam{}}\PY{l+m}{1}
\PY{o}{!}pip\PY{+w}{ }install\PY{+w}{ }gym\PY{o}{[}classic\PYZus{}control\PY{o}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-
packages (69.1.1)
Requirement already satisfied: gym[classic\_control] in
/usr/local/lib/python3.10/dist-packages (0.25.2)
Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-
packages (from gym[classic\_control]) (1.25.2)
Requirement already satisfied: cloudpickle>=1.2.0 in
/usr/local/lib/python3.10/dist-packages (from gym[classic\_control]) (2.2.1)
Requirement already satisfied: gym-notices>=0.0.4 in
/usr/local/lib/python3.10/dist-packages (from gym[classic\_control]) (0.0.8)
Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.10/dist-
packages (from gym[classic\_control]) (2.1.0)
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
\PY{l+s+sd}{A bunch of imports, you don\PYZsq{}t have to worry about these}
\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}

\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{random}
\PY{k+kn}{import} \PY{n+nn}{torch}
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn} \PY{k}{as} \PY{n+nn}{nn}
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn}\PY{n+nn}{.}\PY{n+nn}{functional} \PY{k}{as} \PY{n+nn}{F}
\PY{k+kn}{from} \PY{n+nn}{collections} \PY{k+kn}{import} \PY{n}{namedtuple}\PY{p}{,} \PY{n}{deque}
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{optim} \PY{k}{as} \PY{n+nn}{optim}
\PY{k+kn}{import} \PY{n+nn}{datetime}
\PY{k+kn}{import} \PY{n+nn}{gym}
\PY{k+kn}{from} \PY{n+nn}{gym}\PY{n+nn}{.}\PY{n+nn}{wrappers}\PY{n+nn}{.}\PY{n+nn}{record\PYZus{}video} \PY{k+kn}{import} \PY{n}{RecordVideo}
\PY{k+kn}{import} \PY{n+nn}{glob}
\PY{k+kn}{import} \PY{n+nn}{io}
\PY{k+kn}{import} \PY{n+nn}{base64}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k+kn}{import} \PY{n}{HTML}
\PY{k+kn}{from} \PY{n+nn}{pyvirtualdisplay} \PY{k+kn}{import} \PY{n}{Display}
\PY{k+kn}{import} \PY{n+nn}{tensorflow} \PY{k}{as} \PY{n+nn}{tf}
\PY{k+kn}{from} \PY{n+nn}{IPython} \PY{k+kn}{import} \PY{n}{display} \PY{k}{as} \PY{n}{ipythondisplay}
\PY{k+kn}{from} \PY{n+nn}{PIL} \PY{k+kn}{import} \PY{n}{Image}
\PY{k+kn}{import} \PY{n+nn}{tensorflow\PYZus{}probability} \PY{k}{as} \PY{n+nn}{tfp}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
/usr/local/lib/python3.10/dist-
packages/tensorflow\_probability/python/\_\_init\_\_.py:57: DeprecationWarning:
distutils Version classes are deprecated. Use packaging.version instead.
  if (distutils.version.LooseVersion(tf.\_\_version\_\_) <
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
\PY{l+s+sd}{Please refer to the first tutorial for more details on the specifics of environments}
\PY{l+s+sd}{We\PYZsq{}ve only added important commands you might find useful for experiments.}
\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}

\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
\PY{l+s+sd}{List of example environments}
\PY{l+s+sd}{(Source \PYZhy{} https://gym.openai.com/envs/\PYZsh{}classic\PYZus{}control)}

\PY{l+s+sd}{\PYZsq{}Acrobot\PYZhy{}v1\PYZsq{}}
\PY{l+s+sd}{\PYZsq{}Cartpole\PYZhy{}v1\PYZsq{}}
\PY{l+s+sd}{\PYZsq{}MountainCar\PYZhy{}v0\PYZsq{}}
\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}

\PY{n}{env} \PY{o}{=} \PY{n}{gym}\PY{o}{.}\PY{n}{make}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CartPole\PYZhy{}v1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{env}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}

\PY{n}{state\PYZus{}shape} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{observation\PYZus{}space}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\PY{n}{no\PYZus{}of\PYZus{}actions} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{action\PYZus{}space}\PY{o}{.}\PY{n}{n}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{state\PYZus{}shape}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{no\PYZus{}of\PYZus{}actions}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{env}\PY{o}{.}\PY{n}{action\PYZus{}space}\PY{o}{.}\PY{n}{sample}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
\PY{l+s+sd}{\PYZsh{} Understanding State, Action, Reward Dynamics}

\PY{l+s+sd}{The agent decides an action to take depending on the state.}

\PY{l+s+sd}{The Environment keeps a variable specifically for the current state.}
\PY{l+s+sd}{\PYZhy{} Everytime an action is passed to the environment, it calculates the new state and updates the current state variable.}
\PY{l+s+sd}{\PYZhy{} It returns the new current state and reward for the agent to take the next action}

\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}

\PY{n}{state} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{reset}\PY{p}{(}\PY{p}{)}
\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{} This returns the initial state (when environment is reset) \PYZsq{}\PYZsq{}\PYZsq{}}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{state}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{action} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{action\PYZus{}space}\PY{o}{.}\PY{n}{sample}\PY{p}{(}\PY{p}{)}
\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{} We take a random action now \PYZsq{}\PYZsq{}\PYZsq{}}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{action}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{next\PYZus{}state}\PY{p}{,} \PY{n}{reward}\PY{p}{,} \PY{n}{done}\PY{p}{,} \PY{n}{info} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{n}{action}\PY{p}{)}
\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{} env.step is used to calculate new state and obtain reward based on old state and action taken  \PYZsq{}\PYZsq{}\PYZsq{}}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{next\PYZus{}state}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{reward}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{done}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{info}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
4
2
1
----
[ 0.01369617 -0.02302133 -0.04590265 -0.04834723]
----
0
----
[ 0.01323574 -0.21745604 -0.04686959  0.22950698]
1.0
False
\{\}
----
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283:
DeprecationWarning: `should\_run\_async` will not call `transform\_cell`
automatically in the future. Please pass the result to `transformed\_cell`
argument and any exception that happen during thetransform in
`preprocessing\_exc\_tuple` in IPython 7.17 and above.
  and should\_run\_async(code)
/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning:
\textcolor{ansi-yellow}{WARN: Initializing wrapper in old step API which returns one bool instead
of two. It is recommended to set `new\_step\_api=True` to use new step API. This
will be the default behaviour in future.}
  deprecation(
/usr/local/lib/python3.10/dist-
packages/gym/wrappers/step\_api\_compatibility.py:39: DeprecationWarning:
\textcolor{ansi-yellow}{WARN: Initializing environment in old step API which returns one bool
instead of two. It is recommended to set `new\_step\_api=True` to use new step
API. This will be the default behaviour in future.}
  deprecation(
/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning:
\textcolor{ansi-yellow}{WARN: Function `env.seed(seed)` is marked as deprecated and will be removed
in the future. Please use `env.reset(seed=seed)` instead.}
  deprecation(
/usr/local/lib/python3.10/dist-packages/gym/utils/passive\_env\_checker.py:241:
DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool\_`.
(Deprecated NumPy 1.24)
  if not isinstance(terminated, (bool, np.bool8)):
    \end{Verbatim}

    \hypertarget{dqn}{%
\subsection{DQN}\label{dqn}}

Using NNs as substitutes isn't something new. It has been tried earlier,
but the `human control' paper really popularised using NNs by providing
a few stability ideas (Q-Targets, Experience Replay \& Truncation). The
`Deep-Q Network' (DQN) Algorithm can be broken down into having the
following components.

\hypertarget{q-network}{%
\subsubsection{Q-Network:}\label{q-network}}

The neural network used as a function approximator is defined below

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
\PY{l+s+sd}{\PYZsh{}\PYZsh{}\PYZsh{} Q Network \PYZam{} Some \PYZsq{}hyperparameters\PYZsq{}}

\PY{l+s+sd}{QNetwork1:}
\PY{l+s+sd}{Input Layer \PYZhy{} 4 nodes (State Shape) \PYZbs{}}
\PY{l+s+sd}{Hidden Layer 1 \PYZhy{} 128 nodes \PYZbs{}}
\PY{l+s+sd}{Hidden Layer 2 \PYZhy{} 64 nodes \PYZbs{}}
\PY{l+s+sd}{Output Layer \PYZhy{} 2 nodes (Action Space) \PYZbs{}}
\PY{l+s+sd}{Optimizer \PYZhy{} zero\PYZus{}grad()}
\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}

\PY{k+kn}{import} \PY{n+nn}{torch}
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn} \PY{k}{as} \PY{n+nn}{nn}
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn}\PY{n+nn}{.}\PY{n+nn}{functional} \PY{k}{as} \PY{n+nn}{F}


\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
\PY{l+s+sd}{Bunch of Hyper parameters (Which you might have to tune later)}
\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
\PY{n}{BUFFER\PYZus{}SIZE} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(}\PY{l+m+mf}{1e5}\PY{p}{)}  \PY{c+c1}{\PYZsh{} replay buffer size}
\PY{n}{BATCH\PYZus{}SIZE} \PY{o}{=} \PY{l+m+mi}{64}         \PY{c+c1}{\PYZsh{} minibatch size}
\PY{n}{GAMMA} \PY{o}{=} \PY{l+m+mf}{0.99}            \PY{c+c1}{\PYZsh{} discount factor}
\PY{n}{LR} \PY{o}{=} \PY{l+m+mf}{5e\PYZhy{}4}               \PY{c+c1}{\PYZsh{} learning rate}
\PY{n}{UPDATE\PYZus{}EVERY} \PY{o}{=} \PY{l+m+mi}{20}       \PY{c+c1}{\PYZsh{} how often to update the network (When Q target is present)}


\PY{k}{class} \PY{n+nc}{QNetwork1}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}

    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{state\PYZus{}size}\PY{p}{,} \PY{n}{action\PYZus{}size}\PY{p}{,} \PY{n}{seed}\PY{p}{,} \PY{n}{fc1\PYZus{}units}\PY{o}{=}\PY{l+m+mi}{128}\PY{p}{,} \PY{n}{fc2\PYZus{}units}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Initialize parameters and build model.}
\PY{l+s+sd}{        Params}
\PY{l+s+sd}{        ======}
\PY{l+s+sd}{            state\PYZus{}size (int): Dimension of each state}
\PY{l+s+sd}{            action\PYZus{}size (int): Dimension of each action}
\PY{l+s+sd}{            seed (int): Random seed}
\PY{l+s+sd}{            fc1\PYZus{}units (int): Number of nodes in first hidden layer}
\PY{l+s+sd}{            fc2\PYZus{}units (int): Number of nodes in second hidden layer}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{QNetwork1}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{seed} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{manual\PYZus{}seed}\PY{p}{(}\PY{n}{seed}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc1} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{state\PYZus{}size}\PY{p}{,} \PY{n}{fc1\PYZus{}units}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc2} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{fc1\PYZus{}units}\PY{p}{,} \PY{n}{fc2\PYZus{}units}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc3} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{fc2\PYZus{}units}\PY{p}{,} \PY{n}{action\PYZus{}size}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{state}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Build a network that maps state \PYZhy{}\PYZgt{} action values.\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{x} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{relu}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc1}\PY{p}{(}\PY{n}{state}\PY{p}{)}\PY{p}{)}
        \PY{n}{x} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{relu}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc2}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc3}\PY{p}{(}\PY{n}{x}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{replay-buffer}{%
\subsubsection{Replay Buffer:}\label{replay-buffer}}

Recall why we use such a technique.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{random}
\PY{k+kn}{import} \PY{n+nn}{torch}
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{from} \PY{n+nn}{collections} \PY{k+kn}{import} \PY{n}{deque}\PY{p}{,} \PY{n}{namedtuple}

\PY{n}{device} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{device}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cuda:0}\PY{l+s+s2}{\PYZdq{}} \PY{k}{if} \PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{is\PYZus{}available}\PY{p}{(}\PY{p}{)} \PY{k}{else} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cpu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{k}{class} \PY{n+nc}{ReplayBuffer}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Fixed\PYZhy{}size buffer to store experience tuples.\PYZdq{}\PYZdq{}\PYZdq{}}

    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{action\PYZus{}size}\PY{p}{,} \PY{n}{buffer\PYZus{}size}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{seed}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Initialize a ReplayBuffer object.}

\PY{l+s+sd}{        Params}
\PY{l+s+sd}{        ======}
\PY{l+s+sd}{            action\PYZus{}size (int): dimension of each action}
\PY{l+s+sd}{            buffer\PYZus{}size (int): maximum size of buffer}
\PY{l+s+sd}{            batch\PYZus{}size (int): size of each training batch}
\PY{l+s+sd}{            seed (int): random seed}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{action\PYZus{}size} \PY{o}{=} \PY{n}{action\PYZus{}size}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{memory} \PY{o}{=} \PY{n}{deque}\PY{p}{(}\PY{n}{maxlen}\PY{o}{=}\PY{n}{buffer\PYZus{}size}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{n}{batch\PYZus{}size}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{experience} \PY{o}{=} \PY{n}{namedtuple}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Experience}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{field\PYZus{}names}\PY{o}{=}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{state}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{action}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{reward}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{next\PYZus{}state}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{done}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{seed} \PY{o}{=} \PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{n}{seed}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{add}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{state}\PY{p}{,} \PY{n}{action}\PY{p}{,} \PY{n}{reward}\PY{p}{,} \PY{n}{next\PYZus{}state}\PY{p}{,} \PY{n}{done}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Add a new experience to memory.\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{e} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{experience}\PY{p}{(}\PY{n}{state}\PY{p}{,} \PY{n}{action}\PY{p}{,} \PY{n}{reward}\PY{p}{,} \PY{n}{next\PYZus{}state}\PY{p}{,} \PY{n}{done}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{memory}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{e}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{sample}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Randomly sample a batch of experiences from memory.\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{experiences} \PY{o}{=} \PY{n}{random}\PY{o}{.}\PY{n}{sample}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{memory}\PY{p}{,} \PY{n}{k}\PY{o}{=}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{batch\PYZus{}size}\PY{p}{)}

        \PY{n}{states} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{from\PYZus{}numpy}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{vstack}\PY{p}{(}\PY{p}{[}\PY{n}{e}\PY{o}{.}\PY{n}{state} \PY{k}{for} \PY{n}{e} \PY{o+ow}{in} \PY{n}{experiences} \PY{k}{if} \PY{n}{e} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{float}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
        \PY{n}{actions} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{from\PYZus{}numpy}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{vstack}\PY{p}{(}\PY{p}{[}\PY{n}{e}\PY{o}{.}\PY{n}{action} \PY{k}{for} \PY{n}{e} \PY{o+ow}{in} \PY{n}{experiences} \PY{k}{if} \PY{n}{e} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{long}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
        \PY{n}{rewards} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{from\PYZus{}numpy}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{vstack}\PY{p}{(}\PY{p}{[}\PY{n}{e}\PY{o}{.}\PY{n}{reward} \PY{k}{for} \PY{n}{e} \PY{o+ow}{in} \PY{n}{experiences} \PY{k}{if} \PY{n}{e} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{float}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
        \PY{n}{next\PYZus{}states} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{from\PYZus{}numpy}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{vstack}\PY{p}{(}\PY{p}{[}\PY{n}{e}\PY{o}{.}\PY{n}{next\PYZus{}state} \PY{k}{for} \PY{n}{e} \PY{o+ow}{in} \PY{n}{experiences} \PY{k}{if} \PY{n}{e} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{float}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
        \PY{n}{dones} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{from\PYZus{}numpy}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{vstack}\PY{p}{(}\PY{p}{[}\PY{n}{e}\PY{o}{.}\PY{n}{done} \PY{k}{for} \PY{n}{e} \PY{o+ow}{in} \PY{n}{experiences} \PY{k}{if} \PY{n}{e} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{uint8}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{float}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}

        \PY{k}{return} \PY{p}{(}\PY{n}{states}\PY{p}{,} \PY{n}{actions}\PY{p}{,} \PY{n}{rewards}\PY{p}{,} \PY{n}{next\PYZus{}states}\PY{p}{,} \PY{n}{dones}\PY{p}{)}

    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}len\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Return the current size of internal memory.\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{k}{return} \PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{memory}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{tutorial-agent-code}{%
\subsection{Tutorial Agent Code:}\label{tutorial-agent-code}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{TutorialAgent}\PY{p}{(}\PY{p}{)}\PY{p}{:}

    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{state\PYZus{}size}\PY{p}{,} \PY{n}{action\PYZus{}size}\PY{p}{,} \PY{n}{seed}\PY{p}{)}\PY{p}{:}

\PY{+w}{        }\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{} Agent Environment Interaction \PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{state\PYZus{}size} \PY{o}{=} \PY{n}{state\PYZus{}size}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{action\PYZus{}size} \PY{o}{=} \PY{n}{action\PYZus{}size}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{seed} \PY{o}{=} \PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{n}{seed}\PY{p}{)}

\PY{+w}{        }\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{} Q\PYZhy{}Network \PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{qnetwork\PYZus{}local} \PY{o}{=} \PY{n}{QNetwork1}\PY{p}{(}\PY{n}{state\PYZus{}size}\PY{p}{,} \PY{n}{action\PYZus{}size}\PY{p}{,} \PY{n}{seed}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{qnetwork\PYZus{}target} \PY{o}{=} \PY{n}{QNetwork1}\PY{p}{(}\PY{n}{state\PYZus{}size}\PY{p}{,} \PY{n}{action\PYZus{}size}\PY{p}{,} \PY{n}{seed}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{optimizer} \PY{o}{=} \PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{qnetwork\PYZus{}local}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{n}{LR}\PY{p}{)}

\PY{+w}{        }\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{} Replay memory \PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{memory} \PY{o}{=} \PY{n}{ReplayBuffer}\PY{p}{(}\PY{n}{action\PYZus{}size}\PY{p}{,} \PY{n}{BUFFER\PYZus{}SIZE}\PY{p}{,} \PY{n}{BATCH\PYZus{}SIZE}\PY{p}{,} \PY{n}{seed}\PY{p}{)}

\PY{+w}{        }\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{} Initialize time step (for updating every UPDATE\PYZus{}EVERY steps)           \PYZhy{}Needed for Q Targets \PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{t\PYZus{}step} \PY{o}{=} \PY{l+m+mi}{0}

    \PY{k}{def} \PY{n+nf}{step}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{state}\PY{p}{,} \PY{n}{action}\PY{p}{,} \PY{n}{reward}\PY{p}{,} \PY{n}{next\PYZus{}state}\PY{p}{,} \PY{n}{done}\PY{p}{)}\PY{p}{:}

\PY{+w}{        }\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{} Save experience in replay memory \PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{memory}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{state}\PY{p}{,} \PY{n}{action}\PY{p}{,} \PY{n}{reward}\PY{p}{,} \PY{n}{next\PYZus{}state}\PY{p}{,} \PY{n}{done}\PY{p}{)}

\PY{+w}{        }\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{} If enough samples are available in memory, get random subset and learn \PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{memory}\PY{p}{)} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{n}{BATCH\PYZus{}SIZE}\PY{p}{:}
            \PY{n}{experiences} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{memory}\PY{o}{.}\PY{n}{sample}\PY{p}{(}\PY{p}{)}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{learn}\PY{p}{(}\PY{n}{experiences}\PY{p}{,} \PY{n}{GAMMA}\PY{p}{)}

\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} +Q TARGETS PRESENT \PYZdq{}\PYZdq{}\PYZdq{}}
\PY{+w}{        }\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{} Updating the Network every \PYZsq{}UPDATE\PYZus{}EVERY\PYZsq{} steps taken \PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{t\PYZus{}step} \PY{o}{=} \PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{t\PYZus{}step} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)} \PY{o}{\PYZpc{}} \PY{n}{UPDATE\PYZus{}EVERY}
        \PY{k}{if} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{t\PYZus{}step} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}

            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{qnetwork\PYZus{}target}\PY{o}{.}\PY{n}{load\PYZus{}state\PYZus{}dict}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{qnetwork\PYZus{}local}\PY{o}{.}\PY{n}{state\PYZus{}dict}\PY{p}{(}\PY{p}{)}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{act}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{state}\PY{p}{,} \PY{n}{eps}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{tau}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{policy}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:}

        \PY{n}{state} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{from\PYZus{}numpy}\PY{p}{(}\PY{n}{state}\PY{p}{)}\PY{o}{.}\PY{n}{float}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{unsqueeze}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{qnetwork\PYZus{}local}\PY{o}{.}\PY{n}{eval}\PY{p}{(}\PY{p}{)}
        \PY{k}{with} \PY{n}{torch}\PY{o}{.}\PY{n}{no\PYZus{}grad}\PY{p}{(}\PY{p}{)}\PY{p}{:}
            \PY{n}{action\PYZus{}values} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{qnetwork\PYZus{}local}\PY{p}{(}\PY{n}{state}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{qnetwork\PYZus{}local}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{p}{)}

        \PY{k}{if} \PY{n}{policy} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epsilon\PYZhy{}greedy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
\PY{+w}{            }\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{} Epsilon\PYZhy{}greedy action selection (Already Present) \PYZsq{}\PYZsq{}\PYZsq{}}
            \PY{k}{if} \PY{n}{random}\PY{o}{.}\PY{n}{random}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{n}{eps}\PY{p}{:}
                \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{action\PYZus{}values}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{)}
            \PY{k}{else}\PY{p}{:}
                \PY{k}{return} \PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{action\PYZus{}size}\PY{p}{)}\PY{p}{)}

        \PY{k}{elif} \PY{n}{policy} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
\PY{+w}{            }\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{} Softmax action selection \PYZsq{}\PYZsq{}\PYZsq{}}
            \PY{n}{action\PYZus{}values\PYZus{}reduced} \PY{o}{=} \PY{n}{action\PYZus{}values}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{action\PYZus{}values}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{)}
            \PY{n}{p} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{n}{action\PYZus{}values\PYZus{}reduced}\PY{o}{/}\PY{n}{tau}\PY{p}{)} \PY{o}{/} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{n}{action\PYZus{}values\PYZus{}reduced}\PY{o}{/}\PY{n}{tau}\PY{p}{)}\PY{p}{)}

            \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{prod}\PY{p}{(}\PY{n}{action\PYZus{}values\PYZus{}reduced}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{,} \PY{n}{p}\PY{o}{=}\PY{n}{p}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{)}

        \PY{k}{else}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Unknown policy.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}


    \PY{k}{def} \PY{n+nf}{learn}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{experiences}\PY{p}{,} \PY{n}{gamma}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} +E EXPERIENCE REPLAY PRESENT \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{states}\PY{p}{,} \PY{n}{actions}\PY{p}{,} \PY{n}{rewards}\PY{p}{,} \PY{n}{next\PYZus{}states}\PY{p}{,} \PY{n}{dones} \PY{o}{=} \PY{n}{experiences}

\PY{+w}{        }\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{} Get max predicted Q values (for next states) from target model\PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{n}{Q\PYZus{}targets\PYZus{}next} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{qnetwork\PYZus{}target}\PY{p}{(}\PY{n}{next\PYZus{}states}\PY{p}{)}\PY{o}{.}\PY{n}{detach}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{unsqueeze}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}

\PY{+w}{        }\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{} Compute Q targets for current states \PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{n}{Q\PYZus{}targets} \PY{o}{=} \PY{n}{rewards} \PY{o}{+} \PY{p}{(}\PY{n}{gamma} \PY{o}{*} \PY{n}{Q\PYZus{}targets\PYZus{}next} \PY{o}{*} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{dones}\PY{p}{)}\PY{p}{)}

\PY{+w}{        }\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{} Get expected Q values from local model \PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{n}{Q\PYZus{}expected} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{qnetwork\PYZus{}local}\PY{p}{(}\PY{n}{states}\PY{p}{)}\PY{o}{.}\PY{n}{gather}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{actions}\PY{p}{)}

\PY{+w}{        }\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{} Compute loss \PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{n}{loss} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{mse\PYZus{}loss}\PY{p}{(}\PY{n}{Q\PYZus{}expected}\PY{p}{,} \PY{n}{Q\PYZus{}targets}\PY{p}{)}

\PY{+w}{        }\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{} Minimize the loss \PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{optimizer}\PY{o}{.}\PY{n}{zero\PYZus{}grad}\PY{p}{(}\PY{p}{)}
        \PY{n}{loss}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}

\PY{+w}{        }\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{} Gradiant Clipping \PYZsq{}\PYZsq{}\PYZsq{}}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} +T TRUNCATION PRESENT \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{k}{for} \PY{n}{param} \PY{o+ow}{in} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{qnetwork\PYZus{}local}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{:}
            \PY{n}{param}\PY{o}{.}\PY{n}{grad}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{clamp\PYZus{}}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}

        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{optimizer}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{here-we-present-the-dqn-algorithm-code.}{%
\subsubsection{Here, we present the DQN algorithm
code.}\label{here-we-present-the-dqn-algorithm-code.}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{} Defining DQN Algorithm \PYZsq{}\PYZsq{}\PYZsq{}}

\PY{n}{state\PYZus{}shape} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{observation\PYZus{}space}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\PY{n}{action\PYZus{}shape} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{action\PYZus{}space}\PY{o}{.}\PY{n}{n}


\PY{k}{class} \PY{n+nc}{DQN}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{episode\PYZus{}count} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{episode\PYZus{}times} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{episode\PYZus{}rewards} \PY{o}{=} \PY{p}{[}\PY{p}{]}

    \PY{k}{def} \PY{n+nf}{dqn}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{env}\PY{p}{,} \PY{n}{agent}\PY{p}{,} \PY{n}{n\PYZus{}episodes}\PY{o}{=}\PY{l+m+mi}{10000}\PY{p}{,} \PY{n}{max\PYZus{}t}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{,} \PY{n}{policy}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{param\PYZus{}start}\PY{o}{=}\PY{l+m+mf}{1.0}\PY{p}{,} \PY{n}{param\PYZus{}end}\PY{o}{=}\PY{l+m+mf}{0.01}\PY{p}{,} \PY{n}{param\PYZus{}decay}\PY{o}{=}\PY{l+m+mf}{0.995}\PY{p}{)}\PY{p}{:}

        \PY{n}{scores\PYZus{}window} \PY{o}{=} \PY{n}{deque}\PY{p}{(}\PY{n}{maxlen}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}
\PY{+w}{        }\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{} last 100 scores for checking if the avg is more than 195 \PYZsq{}\PYZsq{}\PYZsq{}}

        \PY{n}{param} \PY{o}{=} \PY{n}{param\PYZus{}start}
\PY{+w}{        }\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{} initialize the parameter \PYZsq{}\PYZsq{}\PYZsq{}}

        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{episode\PYZus{}count} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{n}{start\PYZus{}time} \PY{o}{=} \PY{n}{datetime}\PY{o}{.}\PY{n}{datetime}\PY{o}{.}\PY{n}{now}\PY{p}{(}\PY{p}{)}

        \PY{k}{for} \PY{n}{i\PYZus{}episode} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{n\PYZus{}episodes}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
            \PY{n}{state} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{reset}\PY{p}{(}\PY{p}{)}
            \PY{n}{score} \PY{o}{=} \PY{l+m+mi}{0}
            \PY{k}{for} \PY{n}{t} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{max\PYZus{}t}\PY{p}{)}\PY{p}{:}
                \PY{n}{action} \PY{o}{=} \PY{n}{agent}\PY{o}{.}\PY{n}{act}\PY{p}{(}\PY{n}{state}\PY{p}{,} \PY{n}{eps}\PY{o}{=}\PY{n}{param}\PY{p}{,} \PY{n}{tau}\PY{o}{=}\PY{n}{param}\PY{p}{,} \PY{n}{policy}\PY{o}{=}\PY{n}{policy}\PY{p}{)}
                \PY{n}{next\PYZus{}state}\PY{p}{,} \PY{n}{reward}\PY{p}{,} \PY{n}{done}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{n}{action}\PY{p}{)}
                \PY{n}{agent}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{n}{state}\PY{p}{,} \PY{n}{action}\PY{p}{,} \PY{n}{reward}\PY{p}{,} \PY{n}{next\PYZus{}state}\PY{p}{,} \PY{n}{done}\PY{p}{)}

                \PY{n}{state} \PY{o}{=} \PY{n}{next\PYZus{}state}
                \PY{n}{score} \PY{o}{+}\PY{o}{=} \PY{n}{reward}

                \PY{k}{if} \PY{n}{done}\PY{p}{:}
                    \PY{k}{break}
            \PY{n}{episode\PYZus{}time} \PY{o}{=} \PY{p}{(}\PY{n}{datetime}\PY{o}{.}\PY{n}{datetime}\PY{o}{.}\PY{n}{now}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{start\PYZus{}time}\PY{p}{)}\PY{o}{.}\PY{n}{total\PYZus{}seconds}\PY{p}{(}\PY{p}{)}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{episode\PYZus{}times}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{episode\PYZus{}time}\PY{p}{)}

            \PY{n}{scores\PYZus{}window}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{score}\PY{p}{)}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{episode\PYZus{}rewards}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{score}\PY{p}{)}

            \PY{n}{param} \PY{o}{=} \PY{n+nb}{max}\PY{p}{(}\PY{n}{param\PYZus{}end}\PY{p}{,} \PY{n}{param\PYZus{}decay} \PY{o}{*}\PY{n}{param}\PY{p}{)}
\PY{+w}{            }\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{} decrease the parameter \PYZsq{}\PYZsq{}\PYZsq{}}

            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}r}\PY{l+s+s1}{Episode }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{Average Score: }\PY{l+s+si}{\PYZob{}:.2f\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{i\PYZus{}episode}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{scores\PYZus{}window}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{end}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

            \PY{k}{if} \PY{n}{i\PYZus{}episode} \PY{o}{\PYZpc{}} \PY{l+m+mi}{100} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
              \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}r}\PY{l+s+s1}{Episode }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{Average Score: }\PY{l+s+si}{\PYZob{}:.2f\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{i\PYZus{}episode}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{scores\PYZus{}window}\PY{p}{)}\PY{p}{)}\PY{p}{)}

            \PY{k}{if} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{scores\PYZus{}window}\PY{p}{)}\PY{o}{\PYZgt{}}\PY{o}{=}\PY{l+m+mf}{195.0}\PY{p}{:}
              \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{Environment solved in }\PY{l+s+si}{\PYZob{}:d\PYZcb{}}\PY{l+s+s1}{ episodes!}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{Average Score: }\PY{l+s+si}{\PYZob{}:.2f\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{i\PYZus{}episode}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{scores\PYZus{}window}\PY{p}{)}\PY{p}{)}\PY{p}{)}
              \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{episode\PYZus{}count} \PY{o}{=} \PY{n}{i\PYZus{}episode}
              \PY{k}{break}

        \PY{k}{return} \PY{k+kc}{True}

    \PY{k}{def} \PY{n+nf}{plot\PYZus{}reward\PYZus{}vs\PYZus{}episode}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{episode\PYZus{}rewards}\PY{p}{)}\PY{p}{:}
        \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{episode\PYZus{}rewards}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{episode\PYZus{}rewards}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Reward vs Episode}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Episode}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Reward}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{k+kc}{True}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{plot\PYZus{}score\PYZus{}vs\PYZus{}episode}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{episode\PYZus{}rewards}\PY{p}{,} \PY{n}{window\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}\PY{p}{:}
        \PY{n}{scores} \PY{o}{=} \PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{episode\PYZus{}rewards}\PY{p}{[}\PY{n+nb}{max}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{i} \PY{o}{\PYZhy{}} \PY{n}{window\PYZus{}size}\PY{p}{)}\PY{p}{:}\PY{n}{i} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{episode\PYZus{}rewards}\PY{p}{)}\PY{p}{)}\PY{p}{]}
        \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{scores}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{scores}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Score vs Episode (Moving Average \PYZhy{} Window Size: }\PY{l+s+si}{\PYZob{}}\PY{n}{window\PYZus{}size}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Episode}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Score (Moving Average)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{k+kc}{True}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{plot\PYZus{}time\PYZus{}taken\PYZus{}vs\PYZus{}episode}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{time\PYZus{}taken}\PY{p}{)}\PY{p}{:}
        \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{episode\PYZus{}times}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{episode\PYZus{}times}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Cumulative Time Taken vs Episode}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Episode}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Cumulative Time Taken (seconds)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{k+kc}{True}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}

\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{} Trial run to check if algorithm runs and saves the data \PYZsq{}\PYZsq{}\PYZsq{}}

\PY{n}{begin\PYZus{}time} \PY{o}{=} \PY{n}{datetime}\PY{o}{.}\PY{n}{datetime}\PY{o}{.}\PY{n}{now}\PY{p}{(}\PY{p}{)}
\PY{n}{agent} \PY{o}{=} \PY{n}{TutorialAgent}\PY{p}{(}\PY{n}{state\PYZus{}size}\PY{o}{=}\PY{n}{state\PYZus{}shape}\PY{p}{,}\PY{n}{action\PYZus{}size} \PY{o}{=} \PY{n}{action\PYZus{}shape}\PY{p}{,}\PY{n}{seed} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{)}

\PY{n}{dqn\PYZus{}} \PY{o}{=} \PY{n}{DQN}\PY{p}{(}\PY{p}{)}
\PY{n}{dqn\PYZus{}}\PY{o}{.}\PY{n}{dqn}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{agent}\PY{p}{,} \PY{n}{policy}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epsilon\PYZhy{}greedy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{time\PYZus{}taken} \PY{o}{=} \PY{n}{datetime}\PY{o}{.}\PY{n}{datetime}\PY{o}{.}\PY{n}{now}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{begin\PYZus{}time}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{time\PYZus{}taken}\PY{p}{)}

\PY{n}{dqn\PYZus{}}\PY{o}{.}\PY{n}{plot\PYZus{}reward\PYZus{}vs\PYZus{}episode}\PY{p}{(}\PY{n}{dqn\PYZus{}}\PY{o}{.}\PY{n}{episode\PYZus{}rewards}\PY{p}{)}
\PY{n}{dqn\PYZus{}}\PY{o}{.}\PY{n}{plot\PYZus{}score\PYZus{}vs\PYZus{}episode}\PY{p}{(}\PY{n}{dqn\PYZus{}}\PY{o}{.}\PY{n}{episode\PYZus{}rewards}\PY{p}{)}
\PY{n}{dqn\PYZus{}}\PY{o}{.}\PY{n}{plot\PYZus{}time\PYZus{}taken\PYZus{}vs\PYZus{}episode}\PY{p}{(}\PY{n}{dqn\PYZus{}}\PY{o}{.}\PY{n}{episode\PYZus{}times}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Episode 100     Average Score: 40.87
Episode 200     Average Score: 145.77
Episode 249     Average Score: 195.45
Environment solved in 249 episodes!     Average Score: 195.45
0:01:27.044289
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_11_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_11_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_11_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{task-1a}{%
\subsubsection{\texorpdfstring{\textbf{Task
1a}}{Task 1a}}\label{task-1a}}

Understand the core of the algorithm, follow the flow of data. Identify
the exploration strategy used.

    Exploration strategy used: \(\epsilon -\)greedy with a decaying
\(\epsilon\)

    \hypertarget{task-1b}{%
\subsubsection{\texorpdfstring{\textbf{Task
1b}}{Task 1b}}\label{task-1b}}

Out of the two exploration strategies discussed in class (\(\epsilon\)-greedy
\& Softmax). Implement the strategy that's not used here.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{begin\PYZus{}time} \PY{o}{=} \PY{n}{datetime}\PY{o}{.}\PY{n}{datetime}\PY{o}{.}\PY{n}{now}\PY{p}{(}\PY{p}{)}
\PY{n}{agent} \PY{o}{=} \PY{n}{TutorialAgent}\PY{p}{(}\PY{n}{state\PYZus{}size}\PY{o}{=}\PY{n}{state\PYZus{}shape}\PY{p}{,}\PY{n}{action\PYZus{}size} \PY{o}{=} \PY{n}{action\PYZus{}shape}\PY{p}{,}\PY{n}{seed} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{)}

\PY{n}{dqn\PYZus{}} \PY{o}{=} \PY{n}{DQN}\PY{p}{(}\PY{p}{)}
\PY{n}{dqn\PYZus{}}\PY{o}{.}\PY{n}{dqn}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{agent}\PY{p}{,} \PY{n}{policy}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{time\PYZus{}taken} \PY{o}{=} \PY{n}{datetime}\PY{o}{.}\PY{n}{datetime}\PY{o}{.}\PY{n}{now}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{begin\PYZus{}time}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{time\PYZus{}taken}\PY{p}{)}

\PY{n}{dqn\PYZus{}}\PY{o}{.}\PY{n}{plot\PYZus{}reward\PYZus{}vs\PYZus{}episode}\PY{p}{(}\PY{n}{dqn\PYZus{}}\PY{o}{.}\PY{n}{episode\PYZus{}rewards}\PY{p}{)}
\PY{n}{dqn\PYZus{}}\PY{o}{.}\PY{n}{plot\PYZus{}score\PYZus{}vs\PYZus{}episode}\PY{p}{(}\PY{n}{dqn\PYZus{}}\PY{o}{.}\PY{n}{episode\PYZus{}rewards}\PY{p}{)}
\PY{n}{dqn\PYZus{}}\PY{o}{.}\PY{n}{plot\PYZus{}time\PYZus{}taken\PYZus{}vs\PYZus{}episode}\PY{p}{(}\PY{n}{dqn\PYZus{}}\PY{o}{.}\PY{n}{episode\PYZus{}times}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Episode 99      Average Score: 195.48
Environment solved in 99 episodes!      Average Score: 195.48
0:00:59.304882
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_15_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_15_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_15_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{task-1c}{%
\subsubsection{\texorpdfstring{\textbf{Task
1c}}{Task 1c}}\label{task-1c}}

How fast does the agent `solve' the environment in terms of the number
of episodes? (Cartpole-v1 defines ``solving'' as getting average reward
of 195.0 over 100 consecutive trials)

How `well' does the agent learn? (reward plot?) The above two are some
`evaluation metrics' you can use to comment on the performance of an
algorithm.

Please compare DQN (using \(\epsilon\)-greedy) with DQN (using softmax).
Think along the lines of `no. of episodes', `reward plots', `compute
time', etc. and add a few comments.

    \begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Under decaying \(\epsilon -\) greedy policy
  \texttt{(start\_value\ =\ 1.0,\ end\_value\ =\ 0.01,\ decay\_parameter\ =\ 0.995)},
  the agent solves the environmnet in 249 episodes with a computing time
  of 0:01:27.044289 by acheiving an average reward of 195.45 over 100
  consecutive trials; whereas, with the decaysing softmax policy with
  the precisely same values for the hyperparameter, it solves the
  environment in just 99 episodes with a computing time of
  0:00:59.304882 by acheiving an average reward of 195.48 over 100
  consecutive trials. The reader/ evaluator may also note the following:
  In some other instances of the code run of the exact same parameters,
  \(\epsilon -\) greedy policy makes the agent solve the environment in
  1636 episodes with a final (100 episodes) average Score of 196.18 in
  0:02:53.820056, whereas the exact same run softmax finishes solving
  the environment solved in 62 episodes with the respective average
  Score of 199.48 in just 0:00:23.227403. The data/ output and plots
  concering this has been attached below. \(\epsilon -\) greedy:
\end{enumerate}

\begin{verbatim}
Episode 100 Average Score: 38.24
Episode 200 Average Score: 147.36
Episode 300 Average Score: 132.00
Episode 400 Average Score: 29.183
Episode 500 Average Score: 9.990
Episode 600 Average Score: 9.59
Episode 700 Average Score: 16.64
Episode 800 Average Score: 9.481
Episode 900 Average Score: 9.39
Episode 1000 Average Score: 90.59
Episode 1100 Average Score: 19.410
Episode 1200 Average Score: 24.28
Episode 1300 Average Score: 172.40
Episode 1400 Average Score: 39.322
Episode 1500 Average Score: 19.29
Episode 1600 Average Score: 48.82
Episode 1636 Average Score: 196.18
Environment solved in 1636 episodes!    Average Score: 196.18
0:02:53.820056
\end{verbatim}

\begin{figure}
\centering
\includegraphics{./epsilon-reward.png}
\caption{epsilon-reward}
\end{figure}

\begin{figure}
\centering
\includegraphics{./epsilon-score.png}
\caption{epsilon-score}
\end{figure}

\includegraphics{./epsilon-time.png} 


Softmax:

\begin{verbatim}
Episode 62  Average Score: 199.48
Environment solved in 62 episodes!  Average Score: 199.48
0:00:23.227403
\end{verbatim}

\begin{figure}
\centering
\includegraphics{./softmax-reward.png}
\caption{softmax-reward}
\end{figure}

\begin{figure}
\centering
\includegraphics{./softmax-score.png}
\caption{softmax-score}
\end{figure}

\begin{figure}
\centering
\includegraphics{./softmax-time.png}
\caption{softmax-time}
\end{figure}

The reason might be that \(\epsilon -\) greedy diverged from the nearby
local optimium and somehow, it got captured by a distance local optimum.
The huge spikes at episode 1000, 1200, 1400 and 1600 speaks on this
behalf.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  The instantenous reward vs episode plot is quite stochastic and full
  of spikes in \(\epsilon -\) greedy policy, whereas the same plot in
  Softmax policy is relatively smoother. Although, we may observe that
  the overall nature of the plot of score (average reward over 100
  consecutive episodes) vs epsidoes is quite similar. There are two
  inferences:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \textbf{Unstable learning:} Too many spikes in the instantenous reward
  vs episodes plot in \(\epsilon -\) greedy policy suggests that the
  agent's learning process is unstable. The agent might struggle to
  converge to a consistent and optimal policy, leading to frequent
  fluctuations in its performance.
\item
  \textbf{Exploration- exploitation trade-off:} Softmax, on the other
  hand, shows relative stable and steady learning. The agent shows an
  optimum balance of exploration and exploitation and does not
  hallucinate. As a result, whether it is the instantenous reward curve
  or the average reward curve, we see relative smoothness and fastness
  in the plot of softmax.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Now, coming to the no. of episodes problem. Yes, softmax performs at
  least twice better than \(\epsilon -\) greedy, solves at least twice
  faster, learns at least twice smoother and uses at least twice less
  computing time than \(\epsilon -\) greedy. These can be described as
  follows:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \textbf{Method:} Epsilon-greedy exploration involves randomly
  selecting an action with probability epsilon, which can lead to more
  random and less focused exploration. On the other hand, Softmax has a
  more adaptive exploration strategy as the temperature parameter allows
  for a smooth transition from exploration to exploitation.
\item
  \textbf{Algorithmic complexity:} The underlying algorithmic complexity
  of the epsilon-greedy strategy, especially in the presence of a
  decaying epsilon schedule, can contribute to variations in the
  learning time.
\item
  \textbf{Tuning errors:} One more possiblity is that may be it has not
  been tuned properly wrt the values concering \(\epsilon -\) greedy.
  Fortunately enough, the exact same parameter values generates better
  results in case of Softmax.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Last but not the least, the computing time. Obviously enough, the
  computing time \(\varpropto\) F(no of episodes), where \(F\) can be
  any montonically increasing function (suggesting non-linearity). Hence
  all the reasons that are appilcable for Softmax consuming lesser
  number of episodes are applicable here as well. One interesting thing
  is to comapre the average computing time per a single episode, which
  is 0.34957545783 seconds/ episode for \(\epsilon -\) greedy and
  0.59903921212 seconds/ episode for softmax. This is an interesting
  behaviour as Softmax on average is taking twice computing time per
  episode than \(\epsilon -\) greedy.\(^1\) The reasons mightbe as
  follows:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \textbf{Computational complexity:} The softmax function involves
  exponentiation and normalization, which can be computationally more
  expensive compared to the simple probability-based decision-making in
  epsilon-greedy. On the otherhand, \(\epsilon -\) greedy is much
  simpler from implementation, usage and computational point of view.
\item
  \textbf{Temperature parameter:} The softmax function includes a
  temperature parameter that determines the level of exploration. If the
  temperature is set too high, softmax might require more computations
  to compute probabilities accurately.
\item
  \textbf{Exploration intensity:} Depending on the temperature parameter
  and exploration strategy, softmax might engage in more intense
  exploration, leading to more evaluations of the softmax function and
  potentially higher computational time.
\end{itemize}

\(^1\) Here we are not comparing the computational aspect concering the
second run, where the \(\epsilon -\) greedy took \(\geqslant\) 1600
episodes, as it seems to escape a local convering point and fell in to a
nearby optimum point, on the other hand Softmax converged towards the
nearby optimum point. Hence the computational comparison does not fair
enough in that instance of the test.

    \hypertarget{submission-steps}{%
\paragraph{\texorpdfstring{\textbf{Submission
Steps}}{Submission Steps}}\label{submission-steps}}

\hypertarget{task-1-add-a-text-cell-with-the-answer.}{%
\paragraph{Task 1: Add a text cell with the
answer.}\label{task-1-add-a-text-cell-with-the-answer.}}

\hypertarget{task-2-add-a-code-cell-below-task-1-solution-and-use-tutorial-agent-code-to-build-your-new-agent-with-a-different-exploration-strategy.}{%
\paragraph{Task 2: Add a code cell below task 1 solution and use
`Tutorial Agent Code' to build your new agent (with a different
exploration
strategy).}\label{task-2-add-a-code-cell-below-task-1-solution-and-use-tutorial-agent-code-to-build-your-new-agent-with-a-different-exploration-strategy.}}

\hypertarget{task-3-add-a-code-cell-below-task-2-solution-running-both-the-agents-to-solve-the-cartpole-v-1-environment-and-add-a-new-text-cell-below-it-with-your-inferences.}{%
\paragraph{Task 3: Add a code cell below task 2 solution running both
the agents to solve the CartPole v-1 environment and add a new text cell
below it with your
inferences.}\label{task-3-add-a-code-cell-below-task-2-solution-running-both-the-agents-to-solve-the-cartpole-v-1-environment-and-add-a-new-text-cell-below-it-with-your-inferences.}}


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
