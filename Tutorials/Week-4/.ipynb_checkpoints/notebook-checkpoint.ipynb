{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6700: Tutorial 3 - Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xEl5AbPCamd5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from enum import Enum\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9tjMH9cgH29"
   },
   "source": [
    "Consider a standard grid world, where only 4 (up, down, left, right) actions are allowed and the agent deterministically moves accordingly, represented as below. Here yellow is the start state and white is the goal state.\n",
    "\n",
    "Say, we define our MDP as:\n",
    "- S: 121 (11 x 11) cells \n",
    "- A: 4 actions (up, down, left, right)\n",
    "- P: Deterministic transition probability\n",
    "- R: -1 at every step\n",
    "- gamma: 0.9\n",
    "\n",
    "Our goal is to find an optimal policy (shown in right).\n",
    "\n",
    "\n",
    "\n",
    "![image-1.png](image-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pLSfisLKgKsT"
   },
   "outputs": [],
   "source": [
    "# Above grid is defined as below:\n",
    "#   - 0 denotes an navigable tile\n",
    "#   - 1 denotes an obstruction/wall\n",
    "#   - 2 denotes the start state \n",
    "#   - 3 denotes an goal state\n",
    "\n",
    "# Note: Here the upper left corner is defined as (0, 0)\n",
    "#       and lower right corner as (m-1, n-1)\n",
    "\n",
    "# Optimal Path: RIGHT RIGHT UP UP LEFT LEFT UP UP UP UP UP UP LEFT LEFT DOWN DOWN LEFT LEFT\n",
    "\n",
    "\n",
    "GRID_WORLD = np.array([\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
    "    [1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1],\n",
    "    [1, 3, 0, 0, 1, 0, 1, 0, 1, 0, 1],\n",
    "    [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1],\n",
    "    [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1],\n",
    "    [1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1],\n",
    "    [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
    "    [1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1],\n",
    "    [1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1],\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbTHAOARUWoC"
   },
   "source": [
    "### Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FZ3Eh_bRkBFt"
   },
   "outputs": [],
   "source": [
    "class Actions(Enum):\n",
    "  UP    = (0, (-1, 0))  # index = 0, (xaxis_move = -1 and yaxis_move = 0)\n",
    "  DOWN  = (1, (1, 0))   # index = 1, (xaxis_move = 1 and yaxis_move = 0) \n",
    "  LEFT  = (2, (0, -1))  # index = 2, (xaxis_move = 0 and yaxis_move = -1)\n",
    "  RIGHT = (3, (0, 1))   # index = 3, (xaxis_move = 0 and yaxis_move = -1)\n",
    "\n",
    "  def get_action_dir(self):\n",
    "    _, direction = self.value\n",
    "    return direction\n",
    "\n",
    "  @property\n",
    "  def index(self):\n",
    "    indx, _ = self.value\n",
    "    return indx\n",
    "\n",
    "  @classmethod\n",
    "  def from_index(cls, index):\n",
    "    action_index_map = {a.index: a for a in cls}\n",
    "    return action_index_map[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BDSO8vMfkkdB",
    "outputId": "2951d358-6906-4b65-831b-0bc6524d4177"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: UP, action_id: 0, direction_to_move: (-1, 0)\n",
      "name: DOWN, action_id: 1, direction_to_move: (1, 0)\n",
      "name: LEFT, action_id: 2, direction_to_move: (0, -1)\n",
      "name: RIGHT, action_id: 3, direction_to_move: (0, 1)\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "0 index action is: UP\n"
     ]
    }
   ],
   "source": [
    "# How to use Action enum\n",
    "for a in Actions:\n",
    "  print(f\"name: {a.name}, action_id: {a.index}, direction_to_move: {a.get_action_dir()}\")\n",
    "\n",
    "print(\"\\n------------------------------------\\n\")\n",
    "\n",
    "# find action enum from index 0\n",
    "a = Actions.from_index(0)\n",
    "print(f\"0 index action is: {a.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b53SkgJlDIt"
   },
   "source": [
    "### Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "u5SyrH9vkepn"
   },
   "outputs": [],
   "source": [
    "class BasePolicy:\n",
    "  def update(self, *args):\n",
    "    pass\n",
    "\n",
    "  def select_action(self, state_id: int) -> int:\n",
    "    raise NotImplemented\n",
    "\n",
    "\n",
    "class DeterministicPolicy(BasePolicy):\n",
    "  def __init__(self, actions: np.ndarray):\n",
    "    # actions: its a 1d array (|S| size) which contains action for each state\n",
    "    self.actions = actions\n",
    "\n",
    "  def update(self, state_id, action_id):\n",
    "    assert state_id < len(self.actions), f\"Invalid state_id {state_id}\"\n",
    "    assert action_id < len(Actions), f\"Invalid action_id {action_id}\"\n",
    "    self.actions[state_id] = action_id\n",
    "\n",
    "  def select_action(self, state_id: int) -> int:\n",
    "    assert state_id < len(self.actions), f\"Invalid state_id {state_id}\"\n",
    "    return self.actions[state_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4I7x4rMlFgp"
   },
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Zt-gPaPOldH0"
   },
   "outputs": [],
   "source": [
    "class Environment:\n",
    "  def __init__(self, grid):\n",
    "    self.grid = grid\n",
    "    m, n = grid.shape\n",
    "    self.num_states = m*n\n",
    "\n",
    "  def xy_to_posid(self, x: int, y: int):\n",
    "    _, n = self.grid.shape\n",
    "    return x*n + y\n",
    "\n",
    "  def posid_to_xy(self, posid: int):\n",
    "    _, n = self.grid.shape\n",
    "    return (posid // n, posid % n)\n",
    "\n",
    "  def isvalid_move(self, x: int, y: int):\n",
    "    m, n = self.grid.shape\n",
    "    return (x >= 0) and (y >= 0) and (x < m) and (y < n) and (self.grid[x, y] != 1)\n",
    "\n",
    "  def find_start_xy(self) -> int:\n",
    "    m, n = self.grid.shape\n",
    "    for x in range(m):\n",
    "      for y in range(n):\n",
    "        if self.grid[x, y] == 2:\n",
    "          return (x, y)\n",
    "    raise Exception(\"Start position not found.\")\n",
    "\n",
    "  def find_path(self, policy: BasePolicy) -> str:\n",
    "    max_steps = 50\n",
    "    steps = 0\n",
    "\n",
    "    P, R = self.get_transition_prob_and_expected_reward()\n",
    "    num_actions, num_states = R.shape\n",
    "    all_possible_state_posids = np.arange(num_states)\n",
    "\n",
    "    path = \"\"\n",
    "    curr_x, curr_y = self.find_start_xy()\n",
    "    while (self.grid[curr_x, curr_y] != 3) and (steps < max_steps):\n",
    "      curr_posid = self.xy_to_posid(curr_x, curr_y)\n",
    "      action_id = policy.select_action(curr_posid)\n",
    "      next_posid = np.random.choice(\n",
    "          all_possible_state_posids, p=P[action_id, curr_posid])\n",
    "      action = Actions.from_index(action_id)\n",
    "      path += f\" {action.name}\"\n",
    "      curr_x, curr_y = self.posid_to_xy(next_posid)\n",
    "      steps += 1\n",
    "    return path\n",
    "\n",
    "  def get_transition_prob_and_expected_reward(self):  # P(s_next | s, a), R(s, a)\n",
    "    m, n = self.grid.shape\n",
    "    num_states = m*n\n",
    "    num_actions = len(Actions)\n",
    "    P = np.zeros((num_actions, num_states, num_states))\n",
    "    R = np.zeros((num_actions, num_states))\n",
    "    for a in Actions:\n",
    "      for x in range(m):\n",
    "        for y in range(n):\n",
    "          xmove_dir, ymove_dir = a.get_action_dir()\n",
    "          xnew, ynew = x + xmove_dir, y + ymove_dir  # find the new co-ordinate after the action a\n",
    "\n",
    "          posid = self.xy_to_posid(x, y)\n",
    "          new_posid = self.xy_to_posid(xnew, ynew)\n",
    "  \n",
    "          \n",
    "          if self.grid[x, y] == 3:\n",
    "            # the current state is a goal state\n",
    "            P[a.index, posid, posid] = 1\n",
    "            R[a.index, posid] = 0\n",
    "          elif (self.grid[x, y] == 1) or (not self.isvalid_move(xnew, ynew)):\n",
    "            # the current state is a block state or the next state is invalid\n",
    "            P[a.index, posid, posid] = 1\n",
    "            R[a.index, posid] = -1\n",
    "          else:\n",
    "            # action a is valid and goes to a new position\n",
    "            P[a.index, posid, new_posid] = 1\n",
    "            R[a.index, posid] = -1\n",
    "    return P, R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXLPFU_DUJMw"
   },
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-2.png](image-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vi5e3FAHKoZY"
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(P: np.ndarray, R: np.ndarray, gamma: float,\n",
    "                      policy: BasePolicy, theta: float,\n",
    "                      init_V: np.ndarray=None):\n",
    "  _, num_states = R.shape\n",
    "\n",
    "  # Please try different starting point for V you will find it will always\n",
    "  # converge to the same V_pi value.\n",
    "  \n",
    "  if init_V is None:\n",
    "    init_V = np.zeros(num_states)\n",
    "    \n",
    "  V = copy.deepcopy(init_V)\n",
    "\n",
    "  delta = 100.0\n",
    "  \n",
    "  while delta > theta:\n",
    "    delta = 0.0\n",
    "    \n",
    "    for state_id in range(num_states):\n",
    "      \n",
    "      action_id = policy.select_action(state_id)\n",
    "      \n",
    "      # Following equation is a different way of writing the same equation given in the slide.\n",
    "      # Note here R is an expected reward term.\n",
    "      v = copy.deepcopy(V[state_id])\n",
    "      V[state_id] = R[action_id, state_id] + gamma * np.dot(P[action_id, state_id], V)\n",
    "      \n",
    "      # YOUR CODE HERE\n",
    "      delta = max(delta, np.abs(v - V[state_id])) # Calculate delta which determines when to terminate the evaluation step\n",
    "      \n",
    "  return V\n",
    "\n",
    "\n",
    "def policy_improvement(P: np.ndarray, R: np.ndarray, gamma: float,\n",
    "                      policy: BasePolicy, V: np.ndarray):\n",
    "  _, num_states = R.shape\n",
    "  policy_stable = True\n",
    "  \n",
    "  for state_id in range(num_states):\n",
    "    \n",
    "    old_action_id = policy.select_action(state_id)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    new_action_id = np.argmax(R[:, state_id] + gamma * np.dot(P[:, state_id], V)) # update new_action_id based on the value function.\n",
    "\n",
    "    policy.update(state_id, new_action_id)\n",
    "    if old_action_id != new_action_id:\n",
    "      policy_stable = False\n",
    "      \n",
    "  return policy_stable\n",
    "\n",
    "\n",
    "def policy_iteration(P: np.ndarray, R: np.ndarray, gamma: float,\n",
    "                    theta: float=1e-3, init_policy: BasePolicy = None):\n",
    "  _, num_states = R.shape\n",
    "\n",
    "  # Please try exploring different policies you will find it will always\n",
    "  # converge to the same optimal policy for valid states.\n",
    "  if init_policy is None:\n",
    "    # Say initial policy = all up actions.\n",
    "    init_policy = DeterministicPolicy(actions=np.zeros(num_states, dtype=int))\n",
    "\n",
    "  # creating a copy of a initial policy\n",
    "  policy = copy.deepcopy(init_policy)\n",
    "  policy_stable = False\n",
    "  \n",
    "  while not policy_stable:\n",
    "    V = policy_evaluation(P, R, gamma, policy, theta)\n",
    "    policy_stable = policy_improvement(P, R, gamma, policy, V)\n",
    "    \n",
    "  return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wn0HFryxUd45"
   },
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-7s2t_ttobMJ"
   },
   "outputs": [],
   "source": [
    "def is_same_optimal_value(V1, V2, diff_theta=1e-3):\n",
    "  diff = np.abs(V1 - V2)\n",
    "  return np.all(diff < diff_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "DFP678PtlhBI"
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "\n",
    "gamma = 0.9\n",
    "theta = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mokqMJvA7tPW"
   },
   "outputs": [],
   "source": [
    "env = Environment(GRID_WORLD)\n",
    "P, R = env.get_transition_prob_and_expected_reward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBOiuj8nlYNB"
   },
   "source": [
    "#### Exercise 1: Using Policy iteration algorithm find the optimal path from start to goal position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BT6iIL3KPxGM",
    "outputId": "e3e57c4e-c471-4d36-b9ae-3d35eda204ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RIGHT RIGHT UP UP LEFT LEFT UP UP UP UP UP UP LEFT LEFT DOWN DOWN LEFT LEFT\n"
     ]
    }
   ],
   "source": [
    "# # Start with random choice of init_policy.\n",
    "# One such choice could be: init_policy = np.ones(env.num_states, dtype=int)\n",
    "init_policy = DeterministicPolicy(actions=np.ones(env.num_states, dtype=int))\n",
    "\n",
    "pitr_policy, pitr_V_star = policy_iteration(P, R, gamma, theta=theta, init_policy=init_policy)\n",
    "pitr_path = env.find_path(pitr_policy)\n",
    "print(pitr_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPAR8sVfpg5x"
   },
   "source": [
    "#### Exercise 2: Using initial guess for V as random values, find the optimal value function using policy evaluation and compare it with the optimal value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VKhx-N2ipe6M",
    "outputId": "d442901b-322f-4777-db1e-3470e39374fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start with random choice of init_V.\n",
    "# One such choice could be: init_V = np.random.randn(env.num_states)\n",
    "# Another choice could be: init_V = 10*np.ones(env.num_states)\n",
    "init_V = 10*np.ones(env.num_states)\n",
    "\n",
    "V_star = policy_evaluation(P, R, gamma, pitr_policy, theta, init_V)\n",
    "is_same_optimal_value(pitr_V_star, V_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0RnWzcMPshoj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To-do: Repeat Exercise 1 with a random Deterministic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RIGHT RIGHT UP UP LEFT LEFT UP UP UP UP UP UP LEFT LEFT DOWN DOWN LEFT LEFT\n"
     ]
    }
   ],
   "source": [
    "# # Start with random choice of init_policy.\n",
    "init_policy = DeterministicPolicy(actions=np.random.randint(0, 4, size=env.num_states, dtype=int))\n",
    "\n",
    "pitr_policy, pitr_V_star = policy_iteration(P, R, gamma, theta=theta, init_policy=init_policy)\n",
    "pitr_path = env.find_path(pitr_policy)\n",
    "print(pitr_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dynamic Programming in RL for Finite-State Full-Information Deterministic Systems:**\n",
    "\n",
    "- **Policy Iteration:**\n",
    "  1. **Policy Evaluation:**\n",
    "     - Iteratively update the value function according to the current policy.\n",
    "     - Update based on the Bellman expectation equation.\n",
    "  2. **Policy Improvement:**\n",
    "     - Once the value function is stable, improve the policy by choosing greedy actions.\n",
    "\n",
    "- **Value Iteration:**\n",
    "  - Iteratively update the value function directly based on the maximum expected future reward.\n",
    "  - Combines policy evaluation and improvement in a single step.\n",
    "\n",
    "**Independence from Initialization:**\n",
    "- The statement that the dynamic programming algorithm is not dependent on the initialization of the value function refers to the convergence properties of the algorithm.\n",
    "- The iterative updates and convergence are based on Bellman equations and principles of dynamic programming.\n",
    "- The final results converge to the true value function regardless of the initial values assigned to the states.\n",
    "\n",
    "**Convergence and Error Threshold:**\n",
    "- Convergence is determined by a specified error criterion or threshold.\n",
    "- The algorithm iteratively updates the value function until changes between consecutive iterations fall below the specified error threshold.\n",
    "- The choice of the error threshold affects the termination condition of the algorithm.\n",
    "- While initialization is independent of the final results, the overall convergence is dependent on the chosen error threshold.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "_4I7x4rMlFgp"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
